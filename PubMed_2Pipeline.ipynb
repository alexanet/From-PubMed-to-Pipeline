{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexanet/From-PubMed-to-Pipeline/blob/main/PubMed_2Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhFbZHn8t-kk"
      },
      "outputs": [],
      "source": [
        "# 1) Install dependencies (run this cell in Colab/Jupyter)\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install openai biopython pandas sentence-transformers faiss-cpu tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "id": "Q52eNHl2y7Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "uCwLd2gh_xG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OOVxdA5ly9LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# STEP 1 — Imports\n",
        "# ===========================================\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from time import sleep\n"
      ],
      "metadata": {
        "id": "uhNbK8aX2Hb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# STEP 2 — Fetch PubMed abstracts via Entrez E-utilities\n",
        "# ===========================================\n",
        "def fetch_pubmed_abstracts(query, max_results=50, email=\"your_email@example.com\"):  #Add your email here\n",
        "    # Step 1: Search for PubMed IDs (PMIDs)\n",
        "    search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "    params = {\n",
        "        \"db\": \"pubmed\",\n",
        "        \"term\": query,\n",
        "        \"retmax\": max_results,\n",
        "        \"retmode\": \"xml\",\n",
        "        \"email\": email\n",
        "    }\n",
        "    r = requests.get(search_url, params=params)\n",
        "    root = ElementTree.fromstring(r.content)\n",
        "    pmids = [id_elem.text for id_elem in root.findall(\".//IdList/Id\")]\n",
        "\n",
        "    # Step 2: Fetch summaries for each PMID (title + abstract)\n",
        "    fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "    abstracts = []\n",
        "    batch_size = 10\n",
        "    for i in range(0, len(pmids), batch_size):\n",
        "        batch_pmids = pmids[i:i+batch_size]\n",
        "        fetch_params = {\n",
        "            \"db\": \"pubmed\",\n",
        "            \"id\": \",\".join(batch_pmids),\n",
        "            \"retmode\": \"xml\",\n",
        "            \"email\": email\n",
        "        }\n",
        "        resp = requests.get(fetch_url, params=fetch_params)\n",
        "        root = ElementTree.fromstring(resp.content)\n",
        "        for article in root.findall(\".//PubmedArticle\"):\n",
        "            title = article.findtext(\".//ArticleTitle\")\n",
        "            abstract_text = \"\"\n",
        "            abstract_nodes = article.findall(\".//AbstractText\")\n",
        "            if abstract_nodes:\n",
        "                abstract_text = \" \".join([node.text for node in abstract_nodes if node.text])\n",
        "            abstracts.append({\"title\": title, \"abstract\": abstract_text})\n",
        "        sleep(0.3)  # NCBI recommends a short pause between requests\n",
        "    return pd.DataFrame(abstracts)\n",
        "print (\"this step is done\")"
      ],
      "metadata": {
        "id": "RXdHRJKw2I1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5d5f2d7"
      },
      "source": [
        "# ===========================================\n",
        "# STEP 3-pre — Add your Hugging Face access Token\n",
        "# ===========================================\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# STEP 3 — Load LLM pipeline (Hugging Face)\n",
        "# ===========================================\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # or another free model\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    max_new_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "R3xlvaLb2L-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# STEP 4 — Define relation extraction & summarization functions\n",
        "# ===========================================\n",
        "def extract_relations_llm(text):\n",
        "    prompt = f\"\"\"\n",
        "Extract biomedical entities (genes, diseases, drugs) and their relations from the text below.\n",
        "Return JSON in the format:\n",
        "[\n",
        "  {{\"entity1\": \"...\", \"entity2\": \"...\", \"relation\": \"...\"}}\n",
        "]\n",
        "\n",
        "Text: {text}\n",
        "\"\"\"\n",
        "    result = generator(prompt, do_sample=False, temperature=0.0)\n",
        "    return result[0][\"generated_text\"].replace(prompt, \"\").strip()\n",
        "\n",
        "def summarize_llm(text):\n",
        "    prompt = f\"Summarize the following biomedical abstract in 2 sentences:\\n\\n{text}\"\n",
        "    result = generator(prompt, do_sample=False, temperature=0.0)\n",
        "    return result[0][\"generated_text\"].replace(prompt, \"\").strip()\n"
      ],
      "metadata": {
        "id": "nyiIXBmq2cSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# STEP 5 — Fetch real PubMed abstracts\n",
        "# ===========================================\n",
        "print(\"Fetching PubMed abstracts for query: cancer biomarker...\")\n",
        "df = fetch_pubmed_abstracts(\"cancer biomarker\", max_results=50, email=\"your-email@example.com\")\n",
        "print(f\"Fetched {len(df)} abstracts.\")\n",
        "\n",
        "# Drop rows with empty abstracts\n",
        "df = df[df[\"abstract\"].str.strip() != \"\"]\n"
      ],
      "metadata": {
        "id": "oIOaFCHiCXYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# STEP 6 — Run relation extraction & summarization (sample subset for speed)\n",
        "# WARNING: LLM calls are slow, so limiting to first 10 for demo\n",
        "# ===========================================\n",
        "subset_df = df.head(10).copy()\n",
        "\n",
        "print(\"Running LLM extraction and summarization (10 abstracts)...\")\n",
        "subset_df[\"relations\"] = subset_df[\"abstract\"].apply(extract_relations_llm)\n",
        "subset_df[\"summary\"] = subset_df[\"abstract\"].apply(summarize_llm)\n",
        "\n",
        "print(subset_df[[\"title\", \"summary\", \"relations\"]])"
      ],
      "metadata": {
        "id": "db-e7NW0CbWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# STEP 6a — uncomment and run this step instead of step 6 if in the step 6 takes more than 15-20min\n",
        "# ===========================================\n",
        "# from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# def process_paper(row):\n",
        "#     relations = extract_relations_llm(row[\"abstract\"])\n",
        "#     summary = summarize_llm(row[\"abstract\"])\n",
        "#     return relations, summary\n",
        "\n",
        "# with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "#     results = list(executor.map(process_paper, [row for _, row in subset_df.iterrows()]))\n",
        "\n",
        "# subset_df[\"relations\"], subset_df[\"summary\"] = zip(*results)\n"
      ],
      "metadata": {
        "id": "i5iWqSnDEnku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# STEP 7 — Setup FAISS semantic search on full dataset\n",
        "# ===========================================\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "abstract_embeddings = embedding_model.encode(df[\"abstract\"].tolist())\n",
        "\n",
        "d = abstract_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(abstract_embeddings)\n"
      ],
      "metadata": {
        "id": "zrrMls70CfPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# STEP 8 — Define semantic search function\n",
        "# ===========================================\n",
        "def search_abstracts(query, top_k=3):\n",
        "    query_emb = embedding_model.encode([query])\n",
        "    distances, indices = index.search(query_emb, top_k)\n",
        "    return df.iloc[indices[0]][[\"title\", \"abstract\"]]\n",
        "\n",
        "print(\"\\nSemantic Search for 'EGFR targeted therapy':\")\n",
        "print(search_abstracts(\"EGFR targeted therapy\"))\n"
      ],
      "metadata": {
        "id": "tO2icLG3CfYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# DONE — Save your enriched dataset\n",
        "# ===========================================\n",
        "subset_df.to_csv(\"pubmed_llm_enriched_subset.csv\", index=False)\n",
        "df.to_csv(\"pubmed_abstracts_full.csv\", index=False)\n",
        "\n",
        "print(\"\\nAll done! Files saved.\")"
      ],
      "metadata": {
        "id": "dD1n5abDCmVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# Step 9 — Intractive dashboard\n",
        "# ===========================================\n",
        "!pip install -q gradio\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def search_and_display(query):\n",
        "    # Search top 5 abstracts\n",
        "    results = search_abstracts(query, top_k=5)\n",
        "\n",
        "    outputs = []\n",
        "    for idx, row in results.iterrows():\n",
        "        title = row[\"title\"]\n",
        "        abstract = row[\"abstract\"]\n",
        "\n",
        "        # Try to find enriched summary and relations from subset_df if available\n",
        "        # (since summary & relations only for subset of 10 abstracts)\n",
        "        enriched = subset_df[subset_df[\"title\"] == title]\n",
        "        if not enriched.empty:\n",
        "            summary = enriched[\"summary\"].values[0]\n",
        "            relations = enriched[\"relations\"].values[0]\n",
        "        else:\n",
        "            summary = \"(No summary available)\"\n",
        "            relations = \"(No relations extracted)\"\n",
        "\n",
        "        outputs.append(f\"### {title}\\n**Summary:** {summary}\\n**Relations:** {relations}\\n\\n---\\n\")\n",
        "\n",
        "    return \"\\n\".join(outputs)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=search_and_display,\n",
        "    inputs=gr.Textbox(label=\"Search PubMed abstracts\"),\n",
        "    outputs=gr.Markdown(label=\"Search Results\"),\n",
        "    title=\"PubMed Biomedical Abstracts Search + Relations\",\n",
        "    description=\"Search 50 PubMed abstracts on 'cancer biomarker' and see LLM-extracted summaries & relations.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "sGcpNI8JmoQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# Helper 1 — If you would like to check out the dataset\n",
        "# ===========================================\n",
        "from Bio import Entrez\n",
        "import pandas as pd\n",
        "\n",
        "# PubMed API setup\n",
        "Entrez.email = \"your_email@example.com\" #Add your email here\n",
        "query = \"lung cancer biomarkers\"\n",
        "handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=5)\n",
        "record = Entrez.read(handle)\n",
        "ids = record[\"IdList\"]\n",
        "\n",
        "# Fetch abstracts\n",
        "handle = Entrez.efetch(db=\"pubmed\", id=ids, rettype=\"abstract\", retmode=\"text\")\n",
        "abstracts = handle.read()\n",
        "\n",
        "print(\"PubMed Query:\", query)\n",
        "print(\"=\"*60)\n",
        "print(abstracts)\n"
      ],
      "metadata": {
        "id": "LSWkJMmSv7Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# Helper 2 — Impact Metrics Visualization (Evidence-Based)\n",
        "# ===========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Evidence-based screening rates from Wallace et al. (2013)\n",
        "low_speed = 30   # abstracts/hour (more complex screening)\n",
        "high_speed = 70  # abstracts/hour (simpler screening)\n",
        "\n",
        "abstracts_processed = len(df)  # adjust to subset_df if running demo\n",
        "\n",
        "# Calculate manual times for both low/high speeds\n",
        "manual_time_low = abstracts_processed / low_speed\n",
        "manual_time_high = abstracts_processed / high_speed\n",
        "\n",
        "# Prototype actual processing time (replace with measured time if known)\n",
        "prototype_time = 1.0  # hours\n",
        "\n",
        "# Calculate savings\n",
        "time_saved_low = manual_time_low - prototype_time\n",
        "time_saved_high = manual_time_high - prototype_time\n",
        "pct_saved_low = (time_saved_low / manual_time_low) * 100\n",
        "pct_saved_high = (time_saved_high / manual_time_high) * 100\n",
        "\n",
        "# Print summary metrics\n",
        "print(f\"Abstracts processed: {abstracts_processed}\")\n",
        "print(f\"Manual time (low speed, 30/h): {manual_time_low:.2f} h\")\n",
        "print(f\"Manual time (high speed, 70/h): {manual_time_high:.2f} h\")\n",
        "print(f\"Prototype time: {prototype_time:.2f} h\")\n",
        "print(f\"Time saved range: {time_saved_low:.2f}–{time_saved_high:.2f} h\")\n",
        "print(f\"Percent reduction: {pct_saved_low:.1f}%–{pct_saved_high:.1f}%\")\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "\n",
        "bars = [\"Manual (Low Speed: 30/h)\", \"Manual (High Speed: 70/h)\", \"Prototype\"]\n",
        "times = [manual_time_low, manual_time_high, prototype_time]\n",
        "colors = [\"#D9534F\", \"#F0AD4E\", \"#5CB85C\"]\n",
        "\n",
        "bar_positions = np.arange(len(bars))\n",
        "ax.bar(bar_positions, times, color=colors, alpha=0.85, label=bars)\n",
        "\n",
        "# Add labels\n",
        "for i, v in enumerate(times):\n",
        "    ax.text(i, v + 0.05, f\"{v:.2f} h\", ha=\"center\", fontsize=10, fontweight='bold')\n",
        "\n",
        "# Style settings\n",
        "ax.set_xticks(bar_positions)\n",
        "ax.set_xticklabels(bars, fontsize=11)\n",
        "ax.set_ylabel(\"Hours to Screen Abstracts\", fontsize=12)\n",
        "ax.set_title(\"Screening Time Comparison – Evidence-Based Range\", fontsize=13, fontweight='bold')\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "\n",
        "# Add legend\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TT_X4j6rc_qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12pHYZv-gJWD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}